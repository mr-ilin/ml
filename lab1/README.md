| Студент | Ильин И.О. |
|---------|--------|
| Группа  | М8О-406Б-19      |

# Задача

1) Реализовать следующие алгоритмы машинного обучения: Linear/ Logistic Regression, SVM, KNN, Naive Bayes в отдельных классах.
2) Данные классы должны наследоваться от BaseEstimator и ClassifierMixin, иметь методы fit и predict.
3) Вы должны организовать весь процесс предобработки, обучения и тестирования с помощью Pipeline.
4) Вы должны настроить гиперпараметры моделей с помощью кросс валидации, вывести и сохранить эти гиперпараметры в файл, вместе с обученными моделями.
5) Проделать аналогично с коробочными решениями.
6) Для каждой модели получить оценки метрик: Confusion Matrix, Accuracy, Recall,
Precision, ROC_AUC curve.
7) Проанализировать полученные результаты и сделать выводы о применимости моделей.
8) Загрузить полученные гиперпараметры модели и обученные модели в формате pickle на гит вместе с jupyter notebook ваших экспериментов.

# Подготовка

В прошлой лабораторной работе мы подготовили данные для подачи их в модели машинного обучения: закодировали категориальные признаки, удалили линейно зависимые признаки, пронормировали величины. Загрузим обработанные таблицы через pd.read_csv().

# Linear Regression (Линейная регрессия)

В работе реализованы сопсоб линейной регрессии на основе аналитического решения. То есть мы можем точно вычислить формулу, по которой будет находится матрица весов.

# Logistic Regression (Логистическая регрессия)

Логистическая регрессия описывает распределение признаков. Лосс функция – кросс- энтропия. На выходе получаем вероятность принадлежности объекта к класс

# SVM (Метод опорных векторов)
SVM или метод опорных векторов отличается от линейной регрессии тем, что мы ищем такую гиперплоскость, которая максимально удалена от каждой группы классов. Таким образом, мы решаем проблему, когда наш объект вблизи границы класса. Для этого достаточно поменять функцию ошибки.

# KNN (Метод k-ближайших соседей)

Для метода k-ближайших соседей мы задаем метрику на пространстве и ищем первые k ближайших соседей к нашему классу. Присваиваем нашему объекту класс, который имеет больше всего соседей, используя обычную евклидову метрику.

# Naive Bayes (Наивный байесовский классификатор)

В основе наивного байесовского классификатора лежит теорема Байеса. Теорема Байеса позволяет переставить местами причину и следствие. Зная, с какой вероятностью причина приводит к некоему событию, теорема позволяет рассчитать вероятность того, что именно эта причина привела к наблюдаемому событию. Алгоритм называется наивным, потому что делается предположение условной независимости.

# Результат работы алгоритмов

## Linear Regression

![](../../ai/vlados_ai/ML-main/Lab1/images/Screenshot%202022-10-28%20at%2013.40.33.png)

## Logistic Regression

![](../../ai/vlados_ai/ML-main/Lab1/images/Screenshot%202022-10-28%20at%2013.40.40.png)

## SVM

![](../../ai/vlados_ai/ML-main/Lab1/images/Screenshot%202022-10-28%20at%2013.40.48.png)

## KNN

![](../../ai/vlados_ai/ML-main/Lab1/images/Screenshot%202022-10-28%20at%2013.41.10.png)

## Наивный байесовский классификатор

![](../../ai/vlados_ai/ML-main/Lab1/images/Screenshot%202022-10-28%20at%2013.41.13.png)

# Вывод


В данной лабораторной работе я познакомился с sklearn и базовыми алгоритмами машинного обучения. Также я попробовал реализовать свои версии этих алгоритмов. Так же я изучил линейные модели классического машинного обучения. В результате работы были реализованы все алгоритмы поставленной задачи. Для каждого алгоритма были подобраны лучшие гиперпараметры. Для этого использовалась функция GridSearchCV. Как показали результаты, ручные реализации базовых классификаторов не сильно хуже тех, что есть в sklearn (а некоторые показали себя даже лучше). Все модели, кроме линейной, дали довольно хорошие результаты. Я думаю, можно существенно поднять accuracy, если мы дополнительно поработаем с данными. А именно выделим больше признаков.